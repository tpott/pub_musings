{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from typing import (Any, Dict, List, Tuple)\n",
    "\n",
    "# Third party libraries\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.io.wavfile\n",
    "import scipy.signal\n",
    "import sklearn\n",
    "import sklearn.tree\n",
    "\n",
    "\n",
    "print('IPython.__version__ = %s' % IPython.__version__)\n",
    "print('matplotlib.__version__ = %s' % matplotlib.__version__)\n",
    "print('numpy.__version__ = %s' % np.__version__)\n",
    "print('pandas.__version__ = %s' % pd.__version__)\n",
    "print('scipy.__version__ = %s' % scipy.__version__)\n",
    "print('sklearn.__version__ = %s' % sklearn.__version__)\n",
    "\n",
    "print('\\nseeding random with %d\\n' % int(\"9fe3ddf4da76a6\", 16))\n",
    "random.seed(int(\"9fe3ddf4da76a6\", 16))\n",
    "\n",
    "with open('/proc/%d/status' % os.getpid(), 'rb') as f:\n",
    "    print(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw audio: ../audios/*.wav\n",
    "# Labels: ../outputs/*.json --> ../labels/*.tsv\n",
    "youtube_ids = set([filename.split('.')[0] for filename in os.listdir('../audios/')])\n",
    "training_files = set([\n",
    "    'NrgmdOz227I', # Yesterday\n",
    "])\n",
    "eval_files = set([\n",
    "    'wr2sVPTacTE', # kelly, wife or dog\n",
    "    'TjUXr560Gu0', # dhoom taana\n",
    "])\n",
    "\n",
    "# sampling? first-N-seconds? last-N-seconds? specific time-window?\n",
    "# lazy loading? mono-channel?\n",
    "\n",
    "def sampleData(rate: int, data: np.ndarray, limit_secs: int) -> np.ndarray:\n",
    "    limit = rate * limit_secs  # rate (samples/second) * seconds -> num samples\n",
    "    return data[:limit, 0]\n",
    "\n",
    "\n",
    "def labelsFromUtterances(utterances: List[Dict[str, Any]], windows_per_second: int, n_rows: int) -> Tuple[int, np.ndarray]:\n",
    "    \"\"\"return: np.ndarray[ndtype=intish, shape=[n_rows]]\"\"\"\n",
    "    positive_examples = 0\n",
    "    is_talking = np.zeros(n_rows, dtype=np.int8)\n",
    "    for item in utterances:\n",
    "        # math.ceil rounds up to the latest millisecond for labeling\n",
    "        start_i = int(math.ceil(item['start'] * windows_per_second))\n",
    "        end_i = int(math.ceil(item['end'] * windows_per_second))\n",
    "        for i in range(start_i, min(end_i, n_rows)):\n",
    "            is_talking[i] = 1\n",
    "            positive_examples += 1\n",
    "    return positive_examples, is_talking\n",
    "\n",
    "\n",
    "# TODO use a dataclass instead of a Dict\n",
    "def readData(video_id: str, limit: int) -> Dict[str, Any]:\n",
    "    # dtype should be np.dtype('int16')\n",
    "    rate, all_data = scipy.io.wavfile.read('../audios/%s.wav' % video_id)\n",
    "    data = sampleData(rate, all_data, limit)\n",
    "    \n",
    "    # try wrapping in `int(2 ** math.ceil(math.log(.., 2)))`\n",
    "    window_size = int(rate) // 100\n",
    "    step_size = window_size // 2\n",
    "    # we want windows_per_second to be 200\n",
    "    windows_per_second = int(rate) // step_size\n",
    "    _freqs, _times, spectro = scipy.signal.stft(\n",
    "        data,\n",
    "        rate,\n",
    "        window='hann', # default, as specified by the documentation\n",
    "        nperseg=window_size,\n",
    "        noverlap=window_size // 2\n",
    "    )\n",
    "    \n",
    "    utterances = []\n",
    "    with open('../tsvs/%s.tsv' % video_id, 'rb') as f:\n",
    "        for line in f:\n",
    "            cols = [s.decode('utf-8') for s in line.rstrip(b'\\n').split(b'\\t')]\n",
    "            utterances.append({\n",
    "                'start': float(cols[0]),\n",
    "                'end': float(cols[1]),\n",
    "                'duration': float(cols[2]),\n",
    "                'content': cols[3],\n",
    "            })\n",
    "    \n",
    "    _num_examples, is_talking = labelsFromUtterances(\n",
    "        utterances, \n",
    "        windows_per_second, \n",
    "        spectro.T.shape[0]\n",
    "    )\n",
    "    is_talking = is_talking[:-1].copy()\n",
    "    was_talking = np.hstack(([0], is_talking[:-1]))\n",
    "    was_was_talking = np.hstack(([0, 0], is_talking[:-2]))\n",
    "    \n",
    "    # TODO understand stft input and output shapes\n",
    "    # Drop the last frame because I don't know how it's derived...\n",
    "    return {\n",
    "        'file_name': video_id,\n",
    "        'signal_rate': rate,\n",
    "        'window_size': window_size,\n",
    "        'step_size': step_size,\n",
    "        'num_utterances': len(utterances),\n",
    "        'data': data, # TODO remove this line\n",
    "        'freqs_vec': spectro.T[:-1, :-1],\n",
    "        'is_talking': is_talking,\n",
    "        'was_talking': was_talking,\n",
    "        'was_was_talking': was_was_talking,\n",
    "        # TODO phoneme\n",
    "    }\n",
    "\n",
    "\n",
    "def dict2packed(data: Dict[str, Any]) -> pd.DataFrame:\n",
    "    num_rows = data['freqs_vec'].shape[0]\n",
    "    step_size = data['step_size']  # type: int\n",
    "    window_size = data['window_size']  # type: int\n",
    "    frames = []\n",
    "    for i in range(0, data['data'].shape[0], step_size):\n",
    "        # Cast to lists because pandas doesn't allow numpy.ndarray in cells\n",
    "        frames.append(list(data['data'][i : i + window_size]))\n",
    "    return pd.DataFrame(data={\n",
    "        'file_name': [data['file_name']] * num_rows,\n",
    "        'signal_rate': [data['signal_rate']] * num_rows,\n",
    "        'window_size': [window_size] * num_rows,\n",
    "        'step_size': [step_size] * num_rows,\n",
    "        'num_utterances': [data['num_utterances']] * num_rows,\n",
    "        'raw_signal_vec': frames,\n",
    "        'freqs_vec': data['freqs_vec'].tolist(),\n",
    "        'is_talking': data['is_talking'],\n",
    "        'was_talking': data['was_talking'],\n",
    "        'was_was_talking': data['was_was_talking'],\n",
    "        # TODO phoneme\n",
    "    })\n",
    "\n",
    "\n",
    "def packed2unpacked(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    assert df.shape[0] > 0, 'at least one row is required'\n",
    "    data = {\n",
    "        'file_name': df['file_name'],\n",
    "        'signal_rate': df['signal_rate'],\n",
    "        'window_size': df['window_size'],\n",
    "        'step_size': df['step_size'],\n",
    "        'num_utterances': df['num_utterances'],\n",
    "        'is_talking': df['is_talking'],\n",
    "        'was_talking': df['was_talking'],\n",
    "        'was_was_talking': df['was_was_talking'],\n",
    "    }\n",
    "    window_size = df['window_size'].iat[0]\n",
    "    for i in range(window_size):\n",
    "        # The ternary operator here shouldn't be necessary. For some reason, the\n",
    "        # last frame has half the signal size compared to all the other rows.\n",
    "        data['raw_signal_vec[%d]' % i] = df['raw_signal_vec'].apply(\n",
    "            lambda vec: vec[i] if i < len(vec) else None\n",
    "        )\n",
    "    step_size = df['step_size'].iat[0]\n",
    "    # TODO understand stft input and output shapes\n",
    "    # Why is frequencies limited to 240 complex values instead of 480?...\n",
    "    for i in range(step_size):\n",
    "        data['freqs_vec[%d]' % i] = df['freqs_vec'].apply(lambda vec: vec[i])\n",
    "    return pd.DataFrame(data=data)\n",
    "\n",
    "\n",
    "def utterancesFromPredictions(min_word_width: int, windows_per_second: int, predictions: np.ndarray) -> List[Dict[str, Any]]:\n",
    "    ones = np.ones(min_word_width)\n",
    "    i = 0\n",
    "    predicted_utterances = []\n",
    "    while i < predictions.shape[0] - min_word_width + 1:\n",
    "        # 0 means ~\"no words are spoken\"\n",
    "        if predictions[i] == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        if (predictions[i : i + min_word_width] != ones).all():\n",
    "            predictions[i] = 0\n",
    "            i += 1\n",
    "            continue\n",
    "        for j in range(i + min_word_width, predictions.shape[0]):\n",
    "            if predictions[j] == 0:\n",
    "                j -= 1\n",
    "                break\n",
    "        predicted_utterances.append({\n",
    "            'start': i / windows_per_second,\n",
    "            'end': j / windows_per_second,\n",
    "            'duration': (j - i) / windows_per_second,\n",
    "            # content is TBD!\n",
    "        })\n",
    "        i = j + 1\n",
    "    return predicted_utterances\n",
    "\n",
    "\n",
    "def normalizeFreqs(freqs: np.ndarray, n_buckets: int) -> np.ndarray:\n",
    "    # TODO scale dtype according to n_buckets\n",
    "    assert n_buckets < 250, 'limit n_buckets to fit in int8'\n",
    "    ret = np.zeros(freqs.shape, dtype=np.int8)\n",
    "    buckets = list(map(lambda n: n / n_buckets, range(1, n_buckets)))\n",
    "    # Take the transpose so each row represents the quantile summaries for\n",
    "    # each column in freqs\n",
    "    quantiles = np.quantile(freqs, buckets, axis=0).T\n",
    "    # Enumerate each column of input\n",
    "    # TODO vectorize this...\n",
    "    for i in range(freqs.shape[1]):\n",
    "        ret[:, i] = np.searchsorted(quantiles[i], freqs[:, i])\n",
    "    return ret\n",
    "\n",
    "\n",
    "# Viewing options:\n",
    "# 1) Signal amplitude\n",
    "# 2) Test signal amplitude (examples: sum(freqs[:5]), sum(freqs[5:10]), ...)\n",
    "# 3) Spectrogram pcolormesh\n",
    "# 4) IPython.display.Audio\n",
    "# 5) JSON utterance labels\n",
    "# 6) Time series labels, i.e. for (1-3), `plt.axvline(x=item['start'], color='#d62728')`, TODO: linewidth=wut?\n",
    "# `%matplotlib notebook` may be handy?\n",
    "\n",
    "# Phoneme Labeler:\n",
    "# For each utterance, view a 3 second window.\n",
    "\n",
    "# want: frames (aka windows) of 10 ms, steps of 5 ms.\n",
    "# This is an example packed datastructure\n",
    "# datastruct: (file_name, frame index, signal_rate (example: 44.1kHz), raw_signal_vec, freqs_vec (further want: speech_vec + background_vec), label, phoneme)\n",
    "example_df = pd.DataFrame(data={\n",
    "    'file_name': ['a', 'a', 'a', 'b', 'b'],\n",
    "    'signal_rate': [44100, 44100, 44100, 44100, 44100],\n",
    "    'window_size': [441, 441, 441, 441, 441],\n",
    "    'step_size': [220, 220, 220, 220, 220],\n",
    "    'frame_index': [0, 1, 2, 0, 1],\n",
    "    'window_max_i': [0, 0, 1, 0, 1],\n",
    "    # These are a bit misleading because their length is 2, but window_size says\n",
    "    # they should be 441.\n",
    "    'raw_signal_vec': [[0, 0], [1, 1], [1, 2], [0, 0], [0, 1]],\n",
    "    'freqs_vec': [[0, 0], [1, 0], [2, 1], [0, 0], [0, 1]],\n",
    "    # TODO: fft(fft(raw_signal)) b/c harmonics. consider librosa's \"pitch class\"\n",
    "    'is_talking': [0, 0, 1, None, None],\n",
    "    'was_talking': [0, 0, 0, 1, None],\n",
    "    'was_was_talking': [0, 0, 0, 0, 1],\n",
    "    'phoneme': [None, None, 'a', None, None],\n",
    "})\n",
    "example_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# readData gives a raw Dict/struct\n",
    "# dict2packed repeats some data, like file_name, to fit into a DataFrame\n",
    "# packed2unpacked makes a separate column for each field.\n",
    "#df = dict2packed(readData(list(training_files)[0]))\n",
    "df = pd.concat([dict2packed(readData(in_file, 60)) for in_file in list(training_files)[:2]])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def showFreqsDistMatrix(freqs_vec: pd.Series) -> None:\n",
    "    # This is how REPET-sim does it\n",
    "    # `norm_freqs_vec = freqs_vec * (1.0 / np.sqrt(np.power(freqs_vec, 2).sum()))`\n",
    "    # `true_divide` means `/` instead of `//`\n",
    "    # `[:, np.newaxis]` adds a new dimension to the shape to allow for broadcasting to work\n",
    "    # out and where are to avoid dividing by zero\n",
    "    freqs_vec = np.abs(np.asarray(freqs_vec.tolist()))\n",
    "    lengths = np.linalg.norm(freqs_vec, axis=1)\n",
    "    norm_freqs_vec = np.true_divide(\n",
    "        freqs_vec,\n",
    "        lengths[:, np.newaxis],\n",
    "        out=np.zeros(freqs_vec.shape, dtype='float64'),\n",
    "        where=(lengths != 0.0)[:, np.newaxis]\n",
    "    )\n",
    "    # Contract: `np.power(norm_freqs_vec[i], 2).sum()` ~== 1, for all values of i.\n",
    "    # Except for values of i that represent a zero vector. See above TODO\n",
    "\n",
    "    sim_mat = np.matmul(norm_freqs_vec, norm_freqs_vec.T)\n",
    "    diff_mat = np.subtract(1.0, sim_mat)\n",
    "\n",
    "    frame_ids = np.arange(diff_mat.shape[0])\n",
    "    plt.pcolormesh(frame_ids, frame_ids, diff_mat)\n",
    "    plt.colorbar()\n",
    "    plt.gcf().set_size_inches([15, 12]) # default is 6 x 4\n",
    "    plt.show()\n",
    "    \n",
    "    # quantiles = np.percentile(diff_mat, list(range(5, 100, 5)))\n",
    "    # near_counts = np.zeros((diff_mat.shape[0]))\n",
    "    # for i in range(diff_mat.shape[0]):\n",
    "    #   for j in range(diff_mat.shape[1]):\n",
    "    #     if diff_mat[i, j] >= quantiles[0]:\n",
    "    #       continue\n",
    "    #     near_counts[i] += 1\n",
    "    # np.percentile(near_counts, list(range(1, 100, 1)))\n",
    "    \n",
    "    return diff_mat\n",
    "\n",
    "\n",
    "# diff_mat = showFreqsDistMatrix(df.freqs_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you want to play with unpacked data...\n",
    "# unpacked_df = packed2unpacked(df)\n",
    "# cols_to_drop = [col for col in unpacked_df.columns.tolist() if col.startswith('raw_signal_vec')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['is_talking', 'was_talking', 'was_was_talking']\n",
    "df[cols].groupby(cols).size().to_frame('num_frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = normalizeFreqs(np.abs(np.asarray(df.freqs_vec.tolist())), 20)\n",
    "\n",
    "classifier = sklearn.tree.DecisionTreeClassifier(\n",
    "    random_state=random.randint(0, 2 ** 32 - 1),\n",
    "    max_depth=5\n",
    ")\n",
    "# `[:, :60]` means take all rows, but only the first 60 columns\n",
    "combined = np.hstack([\n",
    "    df.was_was_talking.to_numpy().reshape(normalized.shape[0], 1),\n",
    "    df.was_talking.to_numpy().reshape(normalized.shape[0], 1),\n",
    "    normalized[:, :60],\n",
    "])\n",
    "model = classifier.fit(combined, df.is_talking)\n",
    "plt.gcf().set_size_inches([48, 16]) # default is 6 x 4\n",
    "sklearn.tree.plot_tree(model, max_depth=3, node_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.concat([dict2packed(readData(in_file, 60)) for in_file in list(eval_files)[:1]])\n",
    "eval_normalized = normalizeFreqs(np.abs(np.asarray(eval_df.freqs_vec.tolist())), 20)\n",
    "\n",
    "predictions = np.zeros(eval_normalized.shape[0], dtype='int8')\n",
    "for i in range(eval_normalized.shape[0]):\n",
    "    if i < 2:\n",
    "        predictions[i] = 0\n",
    "        continue\n",
    "    predictions[i] = model.predict([np.hstack([\n",
    "        predictions[i-2],\n",
    "        predictions[i-1],\n",
    "        eval_normalized[i, :60],\n",
    "    ])])\n",
    "\n",
    "# Compare predictions vs eval_df.is_talking\n",
    "comparison = pd.concat([eval_df.is_talking, pd.Series(predictions, name='predictions')], axis=1)\n",
    "comp_counts = comparison.groupby(['is_talking', 'predictions']).size()\n",
    "print('Predicted %d windows of talking, but expected %d' % (comp_counts.loc[:, 1].sum(), comp_counts.loc[1, :].sum()))\n",
    "print('Predicted %d windows of silence, but expected %d' % (comp_counts.loc[:, 0].sum(), comp_counts.loc[0, :].sum()))\n",
    "\n",
    "# Any prediction shorter than this will be considered noise\n",
    "min_word_width = 8\n",
    "#predictions[:min_word_width] = np.zeros(min_word_width)\n",
    "predicted_utterances = utterancesFromPredictions(\n",
    "    min_word_width,\n",
    "    eval_df.signal_rate.iat[0] / eval_df.step_size.iat[0],\n",
    "    predictions\n",
    ")\n",
    "print('Predicted %d utterances, but actually had %d' % (\n",
    "    len(predicted_utterances),\n",
    "    eval_df.num_utterances.iat[0]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison.groupby(['is_talking', 'predictions']).size().to_frame('num_windows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
