{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from typing import (Any, Dict, List, Tuple)\n",
    "\n",
    "# Third party libraries\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.io.wavfile\n",
    "import scipy.signal\n",
    "import sklearn\n",
    "import sklearn.tree\n",
    "\n",
    "print('IPython.__version__ = %s' % IPython.__version__)\n",
    "print('matplotlib.__version__ = %s' % matplotlib.__version__)\n",
    "print('numpy.__version__ = %s' % np.__version__)\n",
    "print('pandas.__version__ = %s' % pd.__version__)\n",
    "print('scipy.__version__ = %s' % scipy.__version__)\n",
    "print('sklearn.__version__ = %s' % sklearn.__version__)\n",
    "\n",
    "print('\\nseeding random with %d\\n' % int(\"9fe3ddf4da76a6\", 16))\n",
    "random.seed(int(\"9fe3ddf4da76a6\", 16))\n",
    "\n",
    "with open('/proc/%d/status' % os.getpid(), 'rb') as f:\n",
    "    print(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw audio: ../audios/*.wav\n",
    "# Labels: ../outputs/*.json --> ../labels/*.tsv\n",
    "youtube_ids = set([filename.split('.')[0] for filename in os.listdir('../audios/')])\n",
    "training_files = set([\n",
    "    'wr2sVPTacTE', # kelly, wife or dog\n",
    "])\n",
    "eval_files = set([\n",
    "    'TjUXr560Gu0', # dhoom taana\n",
    "])\n",
    "\n",
    "# sampling? first-N-seconds? last-N-seconds? specific time-window?\n",
    "# lazy loading? mono-channel?\n",
    "\n",
    "def sampleData(rate: int, data: np.ndarray) -> np.ndarray:\n",
    "    limit = rate * 10 # rate (samples/second) * seconds -> num samples\n",
    "    return data[:limit, 0]\n",
    "\n",
    "\n",
    "def labelsFromUtterances(utterances: List[Dict[str, Any]], windows_per_second: int, n_rows: int) -> Tuple[int, np.ndarray]:\n",
    "    \"\"\"return: np.ndarray[ndtype=intish, shape=[n_rows]]\"\"\"\n",
    "    positive_examples = 0\n",
    "    labels = np.zeros(n_rows)\n",
    "    for item in utterances:\n",
    "        # math.ceil rounds up to the latest millisecond for labeling\n",
    "        start_i = int(math.ceil(item['start'] * windows_per_second))\n",
    "        end_i = int(math.ceil(item['end'] * windows_per_second))\n",
    "        for i in range(start_i, min(end_i, n_rows)):\n",
    "            labels[i] = 1\n",
    "            positive_examples += 1\n",
    "    return positive_examples, labels\n",
    "\n",
    "\n",
    "# TODO use a dataclass instead of a Dict\n",
    "def readData(video_id: str) -> Dict[str, Any]:\n",
    "    # dtype should be np.dtype('int16')\n",
    "    rate, all_data = scipy.io.wavfile.read('../audios/%s.wav' % video_id)\n",
    "    data = sampleData(rate, all_data)\n",
    "    \n",
    "    # try wrapping in `int(2 ** math.ceil(math.log(.., 2)))`\n",
    "    window_size = int(rate) // 100\n",
    "    step_size = window_size // 2\n",
    "    # we want windows_per_second to be 200\n",
    "    windows_per_second = int(rate) // step_size\n",
    "    _freqs, _times, spectro = scipy.signal.stft(\n",
    "        data,\n",
    "        rate,\n",
    "        window='hann', # default, as specified by the documentation\n",
    "        nperseg=window_size,\n",
    "        noverlap=window_size // 2\n",
    "    )\n",
    "    \n",
    "    utterances = []\n",
    "    with open('../tsvs/%s.tsv' % video_id, 'rb') as f:\n",
    "        for line in f:\n",
    "            cols = [s.decode('utf-8') for s in line.rstrip(b'\\n').split(b'\\t')]\n",
    "            utterances.append({\n",
    "                'start': float(cols[0]),\n",
    "                'end': float(cols[1]),\n",
    "                'duration': float(cols[2]),\n",
    "                'content': cols[3],\n",
    "            })\n",
    "    \n",
    "    _num_examples, labels = labelsFromUtterances(\n",
    "        utterances, \n",
    "        windows_per_second, \n",
    "        spectro.T.shape[0]\n",
    "    )\n",
    "    \n",
    "    # TODO understand stft input and output shapes\n",
    "    # Drop the last frame because I don't know how it's derived...\n",
    "    return {\n",
    "        'file_name': video_id,\n",
    "        'signal_rate': rate,\n",
    "        'window_size': window_size,\n",
    "        'step_size': step_size,\n",
    "        'data': data, # TODO remove this line\n",
    "        'freqs_vec': spectro.T[:-1, :-1],\n",
    "        'labels': labels[:-1],\n",
    "        # TODO phoneme\n",
    "    }\n",
    "\n",
    "\n",
    "def dict2packed(data: Dict[str, Any]) -> pd.DataFrame:\n",
    "    num_rows = data['freqs_vec'].shape[0]\n",
    "    step_size = data['step_size']  # type: int\n",
    "    window_size = data['window_size']  # type: int\n",
    "    frames = []\n",
    "    for i in range(0, data['data'].shape[0], step_size):\n",
    "        # Cast to lists because pandas doesn't allow numpy.ndarray in cells\n",
    "        frames.append(list(data['data'][i : i + window_size]))\n",
    "    return pd.DataFrame(data={\n",
    "        'file_name': [data['file_name']] * num_rows,\n",
    "        'signal_rate': [data['signal_rate']] * num_rows,\n",
    "        'window_size': [window_size] * num_rows,\n",
    "        'step_size': [step_size] * num_rows,\n",
    "        'raw_signal_vec': frames,\n",
    "        'freqs_vec': data['freqs_vec'].tolist(),\n",
    "        'labels': data['labels'],\n",
    "        # TODO phoneme\n",
    "    })\n",
    "\n",
    "\n",
    "def packed2unpacked(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    assert df.shape[0] > 0, 'at least one row is required'\n",
    "    data = {\n",
    "        'file_name': df['file_name'],\n",
    "        'signal_rate': df['signal_rate'],\n",
    "        'window_size': df['window_size'],\n",
    "        'step_size': df['step_size'],\n",
    "        'labels': df['labels'],\n",
    "    }\n",
    "    window_size = df['window_size'].iat[0]\n",
    "    for i in range(window_size):\n",
    "        # The ternary operator here shouldn't be necessary. For some reason, the\n",
    "        # last frame has half the signal size compared to all the other rows.\n",
    "        data['raw_signal_vec[%d]' % i] = df['raw_signal_vec'].apply(\n",
    "            lambda vec: vec[i] if i < len(vec) else None\n",
    "        )\n",
    "    step_size = df['step_size'].iat[0]\n",
    "    # TODO understand stft input and output shapes\n",
    "    # Why is frequencies limited to 240 complex values instead of 480?...\n",
    "    for i in range(step_size):\n",
    "        data['freqs_vec[%d]' % i] = df['freqs_vec'].apply(lambda vec: vec[i])\n",
    "    return pd.DataFrame(data=data)\n",
    "\n",
    "\n",
    "# Viewing options:\n",
    "# 1) Signal amplitude\n",
    "# 2) Test signal amplitude (examples: sum(freqs[:5]), sum(freqs[5:10]), ...)\n",
    "# 3) Spectrogram pcolormesh\n",
    "# 4) IPython.display.Audio\n",
    "# 5) JSON utterance labels\n",
    "# 6) Time series labels, i.e. for (1-3), `plt.axvline(x=item['start'], color='#d62728')`, TODO: linewidth=wut?\n",
    "# `%matplotlib notebook` may be handy?\n",
    "\n",
    "# Phoneme Labeler:\n",
    "# For each utterance, view a 3 second window.\n",
    "\n",
    "# want: frames (aka windows) of 10 ms, steps of 5 ms.\n",
    "# This is an example packed datastructure\n",
    "# datastruct: (file_name, frame index, signal_rate (example: 44.1kHz), raw_signal_vec, freqs_vec (further want: speech_vec + background_vec), label, phoneme)\n",
    "example_df = pd.DataFrame(data={\n",
    "    'file_name': ['a', 'a', 'a', 'b', 'b'],\n",
    "    'signal_rate': [44100, 44100, 44100, 44100, 44100],\n",
    "    'window_size': [441, 441, 441, 441, 441],\n",
    "    'step_size': [220, 220, 220, 220, 220],\n",
    "    'frame_index': [0, 1, 2, 0, 1],\n",
    "    'window_max_i': [0, 0, 1, 0, 1],\n",
    "    # These are a bit misleading because their length is 2, but window_size says\n",
    "    # they should be 441.\n",
    "    'raw_signal_vec': [[0, 0], [1, 1], [1, 2], [0, 0], [0, 1]],\n",
    "    'freqs_vec': [[0, 0], [1, 0], [2, 1], [0, 0], [0, 1]],\n",
    "    # TODO: fft(fft(raw_signal)) b/c harmonics. consider librosa's \"pitch class\"\n",
    "    'label': [0, 0, 1, None, None],\n",
    "    'phoneme': [None, None, 'a', None, None],\n",
    "})\n",
    "example_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readData gives a raw Dict/struct\n",
    "# dict2packed repeats some data, like file_name, to fit into a DataFrame\n",
    "# packed2unpacked makes a separate column for each field.\n",
    "df = dict2packed(readData(list(training_files)[0]))\n",
    "#df = dict2packed(readData(list(eval_files)[0]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_vec = np.abs(np.asarray(df.freqs_vec.tolist()))\n",
    "# TODO handle zero rows better. i.e. silence\n",
    "# This is how REPET-sim does it\n",
    "# `norm_freqs_vec = freqs_vec * (1.0 / np.sqrt(np.power(freqs_vec, 2).sum()))`\n",
    "lengths = np.linalg.norm(freqs_vec, axis=1)\n",
    "# `true_divide` means `/` instead of `//`\n",
    "# `[:, np.newaxis]` adds a new dimension to the shape to allow for broadcasting to work\n",
    "# out and where are to avoid dividing by zero\n",
    "norm_freqs_vec = np.true_divide(\n",
    "    freqs_vec,\n",
    "    lengths[:, np.newaxis],\n",
    "    out=np.zeros(freqs_vec.shape, dtype='float64'),\n",
    "    where=(lengths != 0.0)[:, np.newaxis]\n",
    ")\n",
    "# Contract: `np.power(norm_freqs_vec[i], 2).sum()` ~== 1, for all values of i.\n",
    "# Except for values of i that represent a zero vector. See above TODO\n",
    "\n",
    "sim_mat = np.matmul(norm_freqs_vec, norm_freqs_vec.T)\n",
    "frame_ids = np.arange(norm_freqs_vec.shape[0])\n",
    "plt.pcolormesh(frame_ids, frame_ids, sim_mat)\n",
    "plt.colorbar()\n",
    "plt.gcf().set_size_inches([15, 12]) # default is 6 x 4\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_df = packed2unpacked(df)\n",
    "cols_to_drop = [col for col in unpacked_df.columns.tolist() if col.startswith('raw_signal_vec')]\n",
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
