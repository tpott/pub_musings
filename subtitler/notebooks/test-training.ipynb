{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from typing import (Any, Dict, List, Tuple)\n",
    "\n",
    "# Third party libraries\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.io.wavfile\n",
    "import scipy.signal\n",
    "import sklearn\n",
    "import sklearn.tree\n",
    "\n",
    "print('IPython.__version__ = %s' % IPython.__version__)\n",
    "print('matplotlib.__version__ = %s' % matplotlib.__version__)\n",
    "print('numpy.__version__ = %s' % np.__version__)\n",
    "print('pandas.__version__ = %s' % pd.__version__)\n",
    "print('scipy.__version__ = %s' % scipy.__version__)\n",
    "print('sklearn.__version__ = %s' % sklearn.__version__)\n",
    "\n",
    "print('\\nseeding random with %d\\n' % int(\"9fe3ddf4da76a6\", 16))\n",
    "random.seed(int(\"9fe3ddf4da76a6\", 16))\n",
    "\n",
    "with open('/proc/%d/status' % os.getpid(), 'rb') as f:\n",
    "    print(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw audio: ../audios/*.wav\n",
    "# Labels: ../outputs/*.json --> ../labels/*.tsv\n",
    "youtube_ids = set([filename.split('.')[0] for filename in os.listdir('../audios/')])\n",
    "training_files = set([\n",
    "    'wr2sVPTacTE', # kelly, wife or dog\n",
    "])\n",
    "eval_files = set([\n",
    "    'TjUXr560Gu0', # dhoom taana\n",
    "])\n",
    "\n",
    "# sampling? first-N-seconds? last-N-seconds? specific time-window?\n",
    "# lazy loading? mono-channel?\n",
    "\n",
    "def sampleData(rate: int, data: np.ndarray) -> np.ndarray:\n",
    "    limit = rate * 60 # rate (samples/second) * seconds -> num samples\n",
    "    return data[:limit, 0]\n",
    "\n",
    "\n",
    "def labelsFromUtterances(utterances: List[Dict[str, Any]], windows_per_second: int, n_rows: int) -> Tuple[int, np.ndarray]:\n",
    "    \"\"\"return: np.ndarray[ndtype=intish, shape=[n_rows]]\"\"\"\n",
    "    positive_examples = 0\n",
    "    labels = np.zeros(n_rows)\n",
    "    for item in utterances:\n",
    "        # math.ceil rounds up to the latest millisecond for labeling\n",
    "        start_i = int(math.ceil(item['start'] * windows_per_second))\n",
    "        end_i = int(math.ceil(item['end'] * windows_per_second))\n",
    "        for i in range(start_i, min(end_i, n_rows)):\n",
    "            labels[i] = 1\n",
    "            positive_examples += 1\n",
    "    return positive_examples, labels\n",
    "\n",
    "\n",
    "# TODO use a dataclass instead of a Dict\n",
    "def readData(video_id: str) -> Dict[str, Any]:\n",
    "    # dtype should be np.dtype('int16')\n",
    "    rate, all_data = scipy.io.wavfile.read('../audios/%s.wav' % video_id)\n",
    "    data = sampleData(rate, all_data)\n",
    "    \n",
    "    # try wrapping in `int(2 ** math.ceil(math.log(.., 2)))`\n",
    "    window_size = int(rate) // 100\n",
    "    step_size = window_size // 2\n",
    "    # we want windows_per_second to be 200\n",
    "    windows_per_second = int(rate) // step_size\n",
    "    _freqs, _times, spectro = scipy.signal.stft(\n",
    "        data,\n",
    "        rate,\n",
    "        window='hann', # default, as specified by the documentation\n",
    "        nperseg=window_size,\n",
    "        noverlap=window_size // 2\n",
    "    )\n",
    "    \n",
    "    utterances = []\n",
    "    with open('../tsvs/%s.tsv' % video_id, 'rb') as f:\n",
    "        for line in f:\n",
    "            cols = [s.decode('utf-8') for s in line.rstrip(b'\\n').split(b'\\t')]\n",
    "            utterances.append({\n",
    "                'start': float(cols[0]),\n",
    "                'end': float(cols[1]),\n",
    "                'duration': float(cols[2]),\n",
    "                'content': cols[3],\n",
    "            })\n",
    "    \n",
    "    _num_examples, labels = labelsFromUtterances(\n",
    "        utterances, \n",
    "        windows_per_second, \n",
    "        spectro.T.shape[0]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'file_name': video_id,\n",
    "        'signal_rate': rate,\n",
    "        'window_size': window_size,\n",
    "        'step_size': step_size,\n",
    "        'data': data, # TODO remove this line\n",
    "        'freqs_vec': spectro.T,\n",
    "        'labels': labels,\n",
    "        # TODO phoneme\n",
    "    }\n",
    "\n",
    "# Viewing options:\n",
    "# 1) Signal amplitude\n",
    "# 2) Test signal amplitude (examples: sum(freqs[:5]), sum(freqs[5:10]), ...)\n",
    "# 3) Spectrogram pcolormesh\n",
    "# 4) IPython.display.Audio\n",
    "# 5) JSON utterance labels\n",
    "# 6) Time series labels, i.e. for (1-3), `plt.axvline(x=item['start'], color='#d62728')`, TODO: linewidth=wut?\n",
    "# `%matplotlib notebook` may be handy?\n",
    "\n",
    "# Phoneme Labeler:\n",
    "# For each utterance, view a 3 second window.\n",
    "\n",
    "# want: frames (aka windows) of 10 ms, steps of 5 ms.\n",
    "# datastruct: (file_name, frame index, signal_rate (example: 44.1kHz), raw_signal_vec, freqs_vec (further want: speech_vec + background_vec), label, phoneme)\n",
    "df = pd.DataFrame(data={\n",
    "    'file_name': ['a', 'a', 'a', 'b', 'b'],\n",
    "    'signal_rate': [44100, 44100, 44100, 44100, 44100],\n",
    "    'window_size': [441, 441, 441, 441, 441],\n",
    "    'step_size': [220, 220, 220, 220, 220],\n",
    "    'frame_index': [0, 1, 2, 0, 1],\n",
    "    'window_max_i': [0, 0, 1, 0, 1],\n",
    "    # These are a bit misleading because their length is 2, but window_size says\n",
    "    # they should be 441.\n",
    "    'raw_signal_vec': [[0, 0], [1, 1], [1, 2], [0, 0], [0, 1]],\n",
    "    'freqs_vec': [[0, 0], [1, 0], [2, 1], [0, 0], [0, 1]],\n",
    "    # TODO: fft(fft(raw_signal)) b/c harmonics. consider librosa's \"pitch class\"\n",
    "    'label': [0, 0, 1, None, None],\n",
    "    'phoneme': [None, None, 'a', None, None],\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = readData(list(training_files)[0])\n",
    "print(list(res.keys()))\n",
    "print(res['file_name'])\n",
    "print(res['signal_rate'])\n",
    "print(res['step_size'])\n",
    "print(res['window_size'])\n",
    "# data, labels, freqs_vec\n",
    "print(res['data'][:20])\n",
    "print(res['labels'][:20])\n",
    "print(res['freqs_vec'][0, :20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
